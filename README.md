# 云影猎蛛者-基于Grounding DINO的端云协同射击六足机器人
![image](https://github.com/user-attachments/assets/90377063-cfa7-491c-842c-ccc0a36b71c6)
这篇参考文档写的比较随意，没技术报告里面那么多漂亮话，更多的实际开发调试中总结的经验，语言会比较朴素。
## 设计初衷
一开始只是想简单做个基于YOLO检测+PID控制的射击云台配合一辆小车。后面觉得吃技术老本还是太LOW了，想玩一玩最近比较火的多模态大模型，顺便再把老学长传承的六足机器人给用上，自此，便有了此项目。

一开始在考虑射击的时候，在思考能不能不仅仅就射人，而是我啥都能射，指哪打哪。考虑到应用场景和应用任务可以很复杂，要把各种各样的数据都考虑到，太麻烦了。因此想选择一些具有泛化性、零样本训练的模型。例如很火的Clip模型，不过之前在RDK-X5上也体验过clip模型，只能做到简单的分类，而并非目标检测，所以就继续实验一些视觉大模型。

在使用LLAMA Factory体验了一些视觉大模型后，例如QwenVL2.5，感觉输出的效率还是太慢了，而且每次都需要对大模型的回答进行解析处理，那样效率太低了。在不懈努力下，发现Grounding DINO模型，它是一种文本+图像的双模态模型。在输入文本提示词后，检测图像后直接输出检测框信息，单640*480图像在服务器4080super的单卡上最快可以跑到200ms，相比于其他大模型来说简直快到飞起。因此整个项目都基于该模型作为主体，包括后面的三级级联和端云协同方案。

至于为什么是六足机器人而不是四足机器人，原因有三：一方面是因为现在机器狗做的太火了，成本也比较高。另一方面是因为足数越少，姿态控制越难，机器狗的动作姿态啥的还需要调参，工作量太大。六足冗余度高，走路姿态随便控制就能保持平衡。最后是因为这个六足是老学长传承的，零成本（其实这才是主要原因）
## 功能介绍
整个机器人的功能需要实现自动跟踪射击、机器人移动、网页端远程操控。
前两个功能可以理解，但是为啥是网页端，主要是因为一开始是想做纯嵌入式端的语音控制，加个麦克风，连个讯飞大模型的事，后来考虑了一下应用场景，执行射击任务时应当是不能发出巨大声响的（虽然射击声音和舵机驱动声音已经够大了），至少不能大声密谋吧。所以就改做网页端了，相当于一个远程操控手能实时观测。
## 整体方案设计 
![image](https://github.com/user-attachments/assets/93eedb3b-d30a-4adf-834e-b43cb197d0f0)
实际设计参照上图，三端协同采用的是云服务器端、嵌入式端和网页端。其实更准确的来说，网页端应该算是云服务器端，但从使用者的角度来说，网页端还是应当独立出来比较直观一点。

服务器端用带NVIDIA显卡的电脑即可，主要是因为跑大模型都需要用CUDA加速，当然有些游戏笔记本自带独显的也可以作为服务器使用，而且还穿透内网还比较简单。不过我这里就直接用学校提供的服务器了，至于怎么穿透校园内网后面会说。

网页端就写了一个Web应用程序，当然代码是用AI辅助构建的，我就修改了一些共享参数显示，因为本人不是学计算机出身，是自动化本科，前后端对我来说算是外行技术。

嵌入式端用的开发板分别是树莓派4B、RDK-X5、GD32F303。六足机体用的是幻尔科技家的Spider，它上面自带了树莓派4B作为主控制器，把运动控制的任务分离出来了，就像是人体“小脑”一样。这倒是减轻了RDK-X5的负担。RDK-X5的任务就比较多了，更像是大脑，上通过HTTP网络与云服务器进行通信，实际用的是UDP协议。下还需要与树莓派和GD32完成控制机体，指挥射击等操作。跟踪射击部分用的是GD32单片机完成控制，与RDK-X5使用串口通信，为了接线方便，直接用了USB模拟串口，32这边就用了先进的Type-C接口，32的控制功能主要是通过高低电平控制枪完成射击，通过PWM波控制两个舵机，实现自动跟踪。
### Grounding DINO模型介绍

### 自适应像素-角度数学映射模型

### 机体控制逻辑

### 网络设计

### 网页界面设计

### 硬件设计

## 实验效果

## 代码结构

## 代码使用配置指令
